{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For  Openness  the results are as follows when trained with 67% of the data\n",
      "51.5311355311 %\n",
      "For  Conscientiousness  the results are as follows when trained with 67% of the data\n",
      "49.8461538462 %\n",
      "For  Extraversion  the results are as follows when trained with 67% of the data\n",
      "64.293040293 %\n",
      "For  Agreeableness  the results are as follows when trained with 67% of the data\n",
      "67.4175824176 %\n",
      "For  Neuroticism  the results are as follows when trained with 67% of the data\n",
      "70.5860805861 %\n"
     ]
    }
   ],
   "source": [
    "#Naive Bayes Implementation!\n",
    "from __future__ import division\n",
    "import glob\n",
    "import scipy.stats\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import copy\n",
    "import random \n",
    "import math\n",
    "\n",
    "#Naive Bayesian Classifier:\n",
    "\n",
    "dictAuTranslation = {0 : \"AU01\", 1 : \"AU02\", 2 :\"AU04\", 3:\"AU05\", 4:\"AU06\",5:\"AU07\",6:\"AU09\",7:\"AU10\",8:\"AU12\",9:\"AU14\",10:\"AU15\",\n",
    "                     11:\"AU17\",12:\"AU20\",13:\"AU23\",14:\"AU25\",15:\"AU26\",16:\"AU28\",17:\"AU45\"}\n",
    "\n",
    "dictTraitTranslation = {0 : \"Openness\", 1 : \"Conscientiousness\", 2 : \"Extraversion\", 3 : \"Agreeableness\", 4 : \"Neuroticism\"}\n",
    "#Collect Data\n",
    "photoData = {}\n",
    "\n",
    "def getRoundedPhotoData():\n",
    "    traitFile = open(\"C:/Users/rrenv/Desktop/Level4Project/averageRatings.csv\", \"r\")\n",
    "    for line in traitFile:\n",
    "        line = line[:-1]\n",
    "        values = line.split(\",\")\n",
    "        traitValues = []\n",
    "        if (values[0][:12] not in photoData):\n",
    "            photoData[values[0][:12]] = []\n",
    "            for trait in values[1:]:\n",
    "                traitValues.append(round(float(trait)))\n",
    "            photoData[values[0][:12]].append(traitValues)\n",
    "    traitFile.close()\n",
    "    #print len(photoData)\n",
    "    filesToGoThrough = glob.glob(\"C:/Users/rrenv/Desktop/Level4Project/OpenFace_0.2_win_x64/data/outputFiles/*.pts\")\n",
    "    for file in filesToGoThrough:\n",
    "        auIntensities = []\n",
    "        auActivations = []\n",
    "        headPose = []\n",
    "        filename = file.split(\"/\")[-1][12:][:12]\n",
    "        currentFile = open(file, \"r\")\n",
    "        listOfLines = currentFile.readlines()\n",
    "        for i in range(0,3):\n",
    "            value=listOfLines[74].split(\" \")[i]\n",
    "            if (i == 2):\n",
    "                headPose.append(float(value[0:-1]))\n",
    "            else:\n",
    "                headPose.append(float(value))\n",
    "        photoData[filename].append(headPose)\n",
    "        for i in range(0,17):\n",
    "            value=listOfLines[82+i].split(\" \")[1]\n",
    "            auIntensities.append(float(value[0:-1]))\n",
    "        photoData[filename].append(auIntensities)\n",
    "        for i in range(0,18):\n",
    "            value=listOfLines[102+i].split(\" \")[1]\n",
    "            auActivations.append(float(value))\n",
    "        photoData[filename].append(auActivations)\n",
    "        currentFile.close()\n",
    "\n",
    "getRoundedPhotoData()\n",
    "#print photoData\n",
    "\n",
    "#Make a simple list led by class, whether considered \"high\" or \"low\" for the trait\n",
    "#Only worrying about AUIntensities for now\n",
    "\n",
    "def getMedian(trait):\n",
    "    traitValues = []\n",
    "    for photo in photoData:\n",
    "        traitValues.append(photoData[photo][0][trait])\n",
    "    return np.median(traitValues)\n",
    "\n",
    "def reformatData(trait):\n",
    "    listedData = []\n",
    "    median = getMedian(trait)\n",
    "    for photo in photoData:\n",
    "        if(len(photoData[photo]) == 4): #Might have been some corruption in files..?\n",
    "            classifiedData = []\n",
    "            traitValue = photoData[photo][0][trait]\n",
    "            if traitValue >= median: #Turn into 1 or 0 just to signifiy if high or low for the trait\n",
    "                traitValue = 1\n",
    "            else:\n",
    "                traitValue = 0\n",
    "            classifiedData.append(traitValue)\n",
    "            auIntensities = photoData[photo][2]\n",
    "            for au in auIntensities:\n",
    "                classifiedData.append(au)\n",
    "            listedData.append(classifiedData)\n",
    "        #else:\n",
    "        #    print photo\n",
    "    #print np.amax(listedData)\n",
    "    #print np.amin(listedData)\n",
    "    return listedData\n",
    "\n",
    "formattedData = reformatData(2)\n",
    "#print len(formattedData)\n",
    "        \n",
    "#Split into Training and Testing\n",
    "\n",
    "def splitData(data, ratio):\n",
    "    trainingSize = int(len(data) * ratio)\n",
    "    trainSet = []\n",
    "    copyOfData = copy.copy(data)\n",
    "    while (len(trainSet) < trainingSize):\n",
    "        trainSet.append(copyOfData.pop(random.randint(0,len(copyOfData)-1)))\n",
    "    return [trainSet, copyOfData]\n",
    "\n",
    "trainSet, testSet = splitData(formattedData, 0.05)\n",
    "#print trainSet\n",
    "\n",
    "#Separate Data by Class\n",
    "################################## NAIVE BAYES RESULTS ARE VERY UNINTERESTING, DO NOT USE #######################################\n",
    "\n",
    "def assignHighOrLow(dataset):\n",
    "    assigned = {}\n",
    "    for i in range(len(dataset)):\n",
    "        vector = dataset[i]\n",
    "        if (vector[0] not in assigned):\n",
    "            assigned[vector[0]] = []\n",
    "        assigned[vector[0]].append(vector)\n",
    "    return assigned\n",
    "\n",
    "assignments = assignHighOrLow(trainSet)\n",
    "#print assignments\n",
    "#Get Mean for each Attribute\n",
    "\n",
    "def getMean(feature):\n",
    "    return (sum(feature)/len(feature))\n",
    "\n",
    "#Get Standard Deviation\n",
    "\n",
    "def getStdDev(feature):\n",
    "    mean = getMean(feature)\n",
    "    variance = sum([pow(x-mean,2) for x in feature])/float(len(feature)-1)\n",
    "    return np.sqrt(variance)\n",
    "\n",
    "#Summarise Dataset\n",
    "\n",
    "def getAllFeatures(dataset):\n",
    "    features = {}\n",
    "    for datapoint in dataset:\n",
    "        for x in range(1, len(datapoint)): #since [0] would be the class\n",
    "            if dictAuTranslation[x] in features:\n",
    "                features[dictAuTranslation[x-1]].append(datapoint[x])\n",
    "                #print datapoint[x]\n",
    "            else:\n",
    "                features[dictAuTranslation[x-1]] = []\n",
    "                features[dictAuTranslation[x-1]].append(datapoint[x])\n",
    "    return features\n",
    "\n",
    "allFeatures = getAllFeatures(trainSet)\n",
    "#print allFeatures\n",
    "\n",
    "    \n",
    "def summariseFeatures(dataset):\n",
    "    features = getAllFeatures(dataset)\n",
    "    summaries = {}\n",
    "    for x in range(0,len(features)-1):\n",
    "        mean = getMean(features[dictAuTranslation[x]])\n",
    "        stdDev = getStdDev(features[dictAuTranslation[x]])\n",
    "        summaries[dictAuTranslation[x]] = []\n",
    "        summaries[dictAuTranslation[x]].append(mean)\n",
    "        summaries[dictAuTranslation[x]].append(stdDev)\n",
    "    return summaries\n",
    "\n",
    "summarisedFeatures = summariseFeatures(trainSet)\n",
    "#print summarisedFeatures\n",
    "\n",
    "#Summarise Attributes by Class\n",
    "\n",
    "def summariseClasses(dataset):\n",
    "    assigned = assignHighOrLow(dataset)\n",
    "    summaries = {}\n",
    "    for classValue, instances in assigned.iteritems():\n",
    "        summaries[classValue] = summariseFeatures(instances)\n",
    "    return summaries\n",
    "\n",
    "summarisedClasses = summariseClasses(trainSet)\n",
    "#print summarisedClasses\n",
    "#Make Predictions based on Probabilities\n",
    "\n",
    "#Gaussian Probablilty (probability of a given value given mean and standard deviation)\n",
    "\n",
    "def findprobability(value, mean, stdev): # value is what we're looking for the probability of\n",
    "    exponent = math.exp(-(math.pow(value-mean,2)/(2*math.pow(stdev,2))))\n",
    "    return (1/(math.sqrt(2*math.pi)*stdev))*exponent\n",
    "\n",
    "#Apply to classes in our data\n",
    "\n",
    "def calculateClassProbabilities(summaries, inputVector):\n",
    "    probabilities = {}\n",
    "    for classValue, classSummaries in summaries.iteritems():\n",
    "        probabilities[classValue] = 1\n",
    "        for i in range(1, len(classSummaries)):\n",
    "            mean, stdev = classSummaries[dictAuTranslation[i]]\n",
    "            x = inputVector[i]\n",
    "            if (stdev != 0):\n",
    "                probabilities[classValue] *= findprobability(x, mean, stdev)\n",
    "    return probabilities\n",
    "\n",
    "inputVector = testSet[random.randint(0,len(testSet)-1)]\n",
    "classProbabilities = calculateClassProbabilities(summarisedClasses, inputVector)\n",
    "#print classProbabilities\n",
    "\n",
    "#Start making predictions...\n",
    "\n",
    "def predict(summaries, inputVector):\n",
    "    probabilities = calculateClassProbabilities(summaries, inputVector)\n",
    "    highestProbability = -1\n",
    "    fittingClass = None\n",
    "    for classValue, probability in probabilities.iteritems():\n",
    "        if probability > highestProbability:\n",
    "            highestProbability = probability\n",
    "            fittingClass = classValue\n",
    "    return fittingClass\n",
    "\n",
    "def getPredictions(summaries, testData):\n",
    "    predictions = []\n",
    "    for i in range(len(testData)):\n",
    "        result = predict(summaries, testData[i])\n",
    "        predictions.append(result)\n",
    "    return predictions\n",
    "\n",
    "def getAccuracy(testData, predictions):\n",
    "    numberCorrect = 0\n",
    "    for i in range(len(testData)):\n",
    "        if testData[i][0] == predictions[i]:\n",
    "            numberCorrect += 1\n",
    "    return ((numberCorrect/float(len(testData))) * 100)\n",
    "\n",
    "def getAverageAccuracy(allData):\n",
    "    percentages = []\n",
    "    for i in range(0, 100):#MANIPULATE TO CHANGE TRAINING SIZE\n",
    "        trainingData, testData = splitData(allData, (0.67))\n",
    "        summary = summariseClasses(trainingData)\n",
    "        percentages.append(getAccuracy(testData, getPredictions(summary, testData)))\n",
    "    return getMean(percentages)\n",
    "\n",
    "#Testing with all features taken into account\n",
    "def testing(allData):\n",
    "    for i in range(50,70):#MANIPULATE TO CHANGE TRAINING SIZE\n",
    "        trainingData, testData = splitData(allData, (i/float(100)))\n",
    "        summary = summariseClasses(trainingData)\n",
    "        getAccuracy(testData, getPredictions(summary, testData))\n",
    "        \n",
    "        \n",
    "# i in range(0,5):\n",
    "#    print \"For \", dictTraitTranslation[i], \" the results are as follows when trained between 50 and 70 percent of the data:\"\n",
    "#    formattedData = reformatData(i)\n",
    "#    testing(formattedData)\n",
    "\n",
    "for i in range(0,5):\n",
    "    print \"For \", dictTraitTranslation[i], \" the results are as follows when trained with 67% of the data\"\n",
    "    formattedData = reformatData(i)\n",
    "    print getAverageAccuracy(formattedData),'%'\n",
    "\n",
    "#############################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import glob\n",
    "import itertools\n",
    "import scipy.stats\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import copy\n",
    "import random \n",
    "import math\n",
    "\n",
    "# KNN implementation of some sort\n",
    "\n",
    "dictAuTranslation = {0 : \"AU01\", 1 : \"AU02\", 2 :\"AU04\", 3:\"AU05\", 4:\"AU06\",5:\"AU07\",6:\"AU09\",7:\"AU10\",8:\"AU12\",9:\"AU14\",10:\"AU15\",\n",
    "                     11:\"AU17\",12:\"AU20\",13:\"AU23\",14:\"AU25\",15:\"AU26\",16:\"AU28\",17:\"AU45\"}\n",
    "\n",
    "dictTraitTranslation = {0 : \"Openness\", 1 : \"Conscientiousness\", 2 : \"Extraversion\", 3 : \"Agreeableness\", 4 : \"Neuroticism\"}\n",
    "#Collect Data\n",
    "photoData = {}\n",
    "\n",
    "def getRoundedPhotoData():\n",
    "    traitFile = open(\"C:/Users/rrenv/Desktop/Level4Project/averageRatings.csv\", \"r\")\n",
    "    for line in traitFile:\n",
    "        line = line[:-1]\n",
    "        values = line.split(\",\")\n",
    "        traitValues = []\n",
    "        if (values[0][:12] not in photoData):\n",
    "            photoData[values[0][:12]] = []\n",
    "            for trait in values[1:]:\n",
    "                traitValues.append(round(float(trait)))\n",
    "            photoData[values[0][:12]].append(traitValues)\n",
    "    traitFile.close()\n",
    "    #print len(photoData)\n",
    "    filesToGoThrough = glob.glob(\"C:/Users/rrenv/Desktop/Level4Project/OpenFace_0.2_win_x64/data/outputFiles/*.pts\")\n",
    "    for file in filesToGoThrough:\n",
    "        auIntensities = []\n",
    "        auActivations = []\n",
    "        headPose = []\n",
    "        filename = file.split(\"/\")[-1][12:][:12]\n",
    "        currentFile = open(file, \"r\")\n",
    "        listOfLines = currentFile.readlines()\n",
    "        for i in range(0,3):\n",
    "            value=listOfLines[74].split(\" \")[i]\n",
    "            if (i == 2):\n",
    "                headPose.append(float(value[0:-1]))\n",
    "            else:\n",
    "                headPose.append(float(value))\n",
    "        photoData[filename].append(headPose)\n",
    "        for i in range(0,17):\n",
    "            value=listOfLines[82+i].split(\" \")[1]\n",
    "            auIntensities.append(float(value[0:-1]))\n",
    "        photoData[filename].append(auIntensities)\n",
    "        for i in range(0,18):\n",
    "            value=listOfLines[102+i].split(\" \")[1]\n",
    "            auActivations.append(float(value))\n",
    "        photoData[filename].append(auActivations)\n",
    "        currentFile.close()\n",
    "\n",
    "getRoundedPhotoData()\n",
    "#print photoData\n",
    "\n",
    "#Make a simple list led by class, whether considered \"high\" or \"low\" for the trait\n",
    "#Only worrying about AUIntensities for now\n",
    "\n",
    "def getMedian(trait):\n",
    "    traitValues = []\n",
    "    for photo in photoData:\n",
    "        if (len(photoData[photo]) == 4):\n",
    "            traitValues.append(photoData[photo][0][trait])\n",
    "    return np.median(traitValues)\n",
    "\n",
    "def getMean(feature):\n",
    "    return (sum(feature)/len(feature))\n",
    "\n",
    "def reformatData(trait):\n",
    "    listedData = []\n",
    "    median = getMedian(trait)\n",
    "    for photo in photoData:\n",
    "        if(len(photoData[photo]) == 4): #Might have been some corruption in files..?\n",
    "            classifiedData = []\n",
    "            traitValue = photoData[photo][0][trait]\n",
    "            if traitValue >= median: #Turn into 1 or 0 just to signifiy if high or low for the trait\n",
    "                traitValue = 1\n",
    "            else:\n",
    "                traitValue = 0\n",
    "            classifiedData.append(traitValue)\n",
    "            auIntensities = photoData[photo][2]\n",
    "            for au in auIntensities:\n",
    "                classifiedData.append(au)\n",
    "            listedData.append(classifiedData)\n",
    "    return listedData\n",
    "\n",
    "#formattedData = reformatData(2)\n",
    "\n",
    "\n",
    "\n",
    "# Find the closest K Neighbours\n",
    "def findKNeighbours(k, dataPoint, dataSet):\n",
    "    closestdistances = []\n",
    "    closestKNeighbours  = []\n",
    "    for i in range(0,k):\n",
    "        closestdistances.append(float(\"inf\"))\n",
    "        closestKNeighbours.append(None)\n",
    "    index = 0\n",
    "    for data in dataSet:\n",
    "        largestInList = max(closestdistances)\n",
    "        calculation=0\n",
    "        for x in range(1,len(data)-1):\n",
    "            dim_dist = (data[x] - dataPoint[x])**2\n",
    "            calculation += dim_dist\n",
    "        distance = np.sqrt(calculation)\n",
    "        if ((distance < largestInList) and (distance != 0)):\n",
    "            closestdistances[closestdistances.index(largestInList)] = distance\n",
    "            closestKNeighbours[closestdistances.index(distance)] = data #Value at that index changed!\n",
    "    return closestKNeighbours\n",
    "\n",
    "#testPoint = formattedData[164]\n",
    "#neighbours = findKNeighbours(3, testPoint, formattedData)\n",
    "#print neighbours\n",
    "\n",
    "def checkCorrectness(dataPoint, prediction):\n",
    "    if dataPoint[0] == prediction:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "def LowOrHighKNN(k, dataPoint, dataSet):\n",
    "    numberOfHigh = 0\n",
    "    numberOfLow = 0\n",
    "    value = 0\n",
    "    closestKNeighbours = findKNeighbours(k, dataPoint, dataSet)\n",
    "    for neighbour in closestKNeighbours:\n",
    "        if (neighbour[0] == 1):\n",
    "            numberOfHigh += 1\n",
    "        else:\n",
    "            numberOfLow += 1\n",
    "    if (numberOfHigh > numberOfLow):\n",
    "        value = 1\n",
    "    elif (numberOfLow > numberOfHigh):\n",
    "        value = 0\n",
    "    elif (numberOfLow == numberOfHigh):\n",
    "        value = random.randint(0,1)\n",
    "    return value\n",
    "\n",
    "#print LowOrHighKNN(3, testPoint, formattedData)\n",
    "\n",
    "def getReliabilityKNN(k, trait):\n",
    "    #for x in range (0, len(allData)-1):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    formattedData = reformatData(trait)\n",
    "    for x in range(0, 100): #Change how many trials you'd like to do... Higher numbers makes it VERY slow\n",
    "        y = random.randint(0, len(formattedData)-1)\n",
    "        if(checkCorrectness(formattedData[y], LowOrHighKNN(k, formattedData[y], formattedData)) == True):\n",
    "            correct += 1\n",
    "        total += 1\n",
    "    print k, \" nearest neighbours is correct \", (correct/float(total))*100, \"% of the time\"\n",
    "    return correct\n",
    "\n",
    "#for trait in range (0,5):\n",
    "#    print 'for ', dictTraitTranslation[trait], ':'\n",
    "#    for k in range (1, 101):\n",
    "#        getReliabilityKNN(k, trait)\n",
    "\n",
    "############################## K FOLD CROSS VALIDATION ##########################################\n",
    "\n",
    "def kFoldTrainingSets(data):\n",
    "    trainingSets = []\n",
    "    for i in range (0,10):\n",
    "        trainingSets.append([])\n",
    "    copyOfData = copy.copy(data)\n",
    "    while (len(copyOfData) > 0):\n",
    "        for i in range(0,10):\n",
    "            if (len(copyOfData) > 0):\n",
    "                trainingSets[i].append(copyOfData.pop(random.randint(0,len(copyOfData)-1)))\n",
    "    return trainingSets\n",
    "\n",
    "def kFoldCrossValidation(trait, k):\n",
    "    allData = reformatData(trait)\n",
    "    kFoldSets = kFoldTrainingSets(allData)\n",
    "    listOfPercentages = []\n",
    "    for i in range(0, 10): #10 was discussed as a suitable number for this validation, and is hence hardcoded\n",
    "        testData = kFoldSets.pop(i) #Pops the set we wish to use to test, which is then appended to the beginning later\n",
    "        testingData = list(itertools.chain.from_iterable(kFoldSets)) #Join the rest into one list so it's easier to iterate over\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for x in range(0, len(testData)-1): #test on each element in the data\n",
    "            testPoint = testData[x]\n",
    "            if(checkCorrectness(testPoint, LowOrHighKNN(k, testPoint, testingData)) == True):\n",
    "                correct += 1\n",
    "            total += 1\n",
    "        listOfPercentages.append((correct/float(total))*100)\n",
    "        kFoldSets.insert(0, testData) #Add back into the beginning of our list of sets of data; assuring it's not popped again\n",
    "    accuracy = getMean(listOfPercentages)\n",
    "    #print k, \" nearest neighbours is correct \", accuracy, \"% of the time\"\n",
    "    return accuracy\n",
    "\n",
    "########################################## Gather graph data for finding optimal K value #################################################\n",
    "\n",
    "def gatherGraphData():\n",
    "    listOfTraces = []\n",
    "    for i in range(0,5):\n",
    "        listOfAccuraciesForK = []\n",
    "        for j in range(1,100):\n",
    "            listOfAccuraciesForK.append(kFoldCrossValidation(i, j))\n",
    "        listOfTraces.append(listOfAccuraciesForK)\n",
    "    return listOfTraces\n",
    "\n",
    "#print gatherGraphData()\n",
    "\n",
    "#graphData = gatherGraphData()\n",
    "\n",
    "################################## POSSIBLE OPTIMISATION VIA BACKWARDS ELIMINATION #################################################\n",
    "###################################### NOT FULLY TESTED, LIKELY NOT FUNCTIONAL ################################################\n",
    "\n",
    "def findKNeighboursEliminatingVariable(k, dataPoint, dataSet, relevantFeatures):\n",
    "    closestdistances = []\n",
    "    closestKNeighbours  = []\n",
    "    #print relevantFeatures\n",
    "    for i in range(0,k):\n",
    "        closestdistances.append(float(\"inf\"))\n",
    "        closestKNeighbours.append(None)\n",
    "    index = 0\n",
    "    for data in dataSet:\n",
    "        largestInList = max(closestdistances)\n",
    "        calculation=0\n",
    "        for x in relevantFeatures:\n",
    "            #print x\n",
    "            dim_dist = (data[x] - dataPoint[x])**2\n",
    "            calculation += dim_dist\n",
    "        distance = np.sqrt(calculation)\n",
    "        if ((distance < largestInList)):# and (distance != 0)):\n",
    "            closestdistances[closestdistances.index(largestInList)] = distance\n",
    "            closestKNeighbours[closestdistances.index(distance)] = data #Value at that index changed!\n",
    "    return closestKNeighbours\n",
    "\n",
    "def lowOrHighElimVariable(k, dataPoint, dataSet, relevantFeatures):\n",
    "    numberOfHigh = 0\n",
    "    numberOfLow = 0\n",
    "    value = 0\n",
    "    closestKNeighbours = findKNeighboursEliminatingVariable(k, dataPoint, dataSet, relevantFeatures)\n",
    "    #print closestKNeighbours\n",
    "    for neighbour in closestKNeighbours:\n",
    "        if (neighbour[0] == 1):\n",
    "            numberOfHigh += 1\n",
    "        else:\n",
    "            numberOfLow += 1\n",
    "    if (numberOfHigh > numberOfLow):\n",
    "        value = 1\n",
    "    elif (numberOfLow > numberOfHigh):\n",
    "        value = 0\n",
    "    elif (numberOfHigh == numberOfLow):\n",
    "        value = random.randint(0,1)\n",
    "    return value\n",
    "\n",
    "def getVariableReliabilityKNN(k, relevantFeatures, testData, trait):\n",
    "    #for x in range (0, len(allData)-1):\n",
    "    correct = 0\n",
    "    total = len(testData)\n",
    "    for x in range(0, len(testData)-1): #Change how many trials you'd like to do... Higher numbers makes it VERY slow\n",
    "        if(checkCorrectness(testData[x], lowOrHighElimVariable(k, testData[x], testData, relevantFeatures)) == True):\n",
    "            correct += 1\n",
    "    #print k, \" nearest neighbours is correct \", (correct/float(total))*100, \"% of the time\"\n",
    "    return correct\n",
    "\n",
    "def backwardEliminationRE(trait, k):\n",
    "    testData = reformatData(trait-1)\n",
    "    bestCorrectness = 0\n",
    "    relevantFeatures = []\n",
    "    triedAndTested = []\n",
    "    for i in range(1, len(testData[0])):\n",
    "        relevantFeatures.append(i)\n",
    "    baseCorrectness = getVariableReliabilityKNN(k, relevantFeatures, testData, trait)\n",
    "    #print \"Base correctness to begin: \", baseCorrectness\n",
    "    for x in range (0, len(testData[0])-1):\n",
    "        testing = relevantFeatures.pop(0)\n",
    "        #print relevantFeatures\n",
    "        correctnessWithoutFeature = getVariableReliabilityKNN(k, relevantFeatures, testData, trait)\n",
    "        #print correctnessWithoutFeature\n",
    "        if correctnessWithoutFeature < baseCorrectness:\n",
    "            relevantFeatures.append(testing)\n",
    "            triedAndTested.append(testing)\n",
    "    return relevantFeatures\n",
    "\n",
    "def relevantKFoldCrossValidation(trait, k, relevantFeatures):\n",
    "    allData = reformatData(trait)\n",
    "    kFoldSets = kFoldTrainingSets(allData)\n",
    "    listOfPercentages = []\n",
    "    for i in range(0, 10): #10 was discussed as a suitable number for this validation, and is hence hardcoded\n",
    "        testData = kFoldSets.pop(i) #Pops the set we wish to use to test, which is then appended to the beginning later\n",
    "        testingData = list(itertools.chain.from_iterable(kFoldSets)) #Join the rest into one list so it's easier to iterate over\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for x in range(0, len(testData)-1): #test on each element in the data\n",
    "            testPoint = testData[x]\n",
    "            if(checkCorrectness(testPoint, lowOrHighElimVariable(k, testPoint, testingData, relevantFeatures)) == True):\n",
    "                correct += 1\n",
    "            total += 1\n",
    "        listOfPercentages.append((correct/float(total))*100)\n",
    "        kFoldSets.insert(0, testData) #Add back into the beginning of our list of sets of data; assuring it's not popped again\n",
    "    accuracy = getMean(listOfPercentages)\n",
    "    print k, \" nearest neighbours is correct \", accuracy, \"% of the time\"\n",
    "    return accuracy\n",
    "\n",
    "# Highest points for each feature\n",
    "# Openness: 49\n",
    "# Conscientiousness: 58\n",
    "# Extraversion: 72\n",
    "# Agreeableness: 20\n",
    "# Neuroticism: 29\n",
    "\n",
    "\n",
    "#opennessRelevant = backwardEliminationRE(1, 49)\n",
    "#print 'For Openness, the most relevant features are:'\n",
    "#for trait in opennessRelevant:\n",
    "#    print dictAuTranslation[trait-1],\n",
    "#conscRelevant = backwardEliminationRE(2,58)\n",
    "#print 'For Conscientiousness, the most relevant features are:'\n",
    "#for trait in conscRelevant:\n",
    "#    print dictAuTranslation[trait-1],\n",
    "#extrRelevant = backwardEliminationRE(3,72)\n",
    "#print 'For Extraversion, the most relevant features are:'\n",
    "#for trait in extrRelevant:\n",
    "#aggrRelevant = backwardEliminationRE(4,20)\n",
    "#print 'For Agreeableness, the most relevant features are:'\n",
    "#for trait in aggrRelevant:\n",
    "#    print dictAuTranslation[trait-1],\n",
    "#neuroRelevant = backwardEliminationRE(5,29)\n",
    "#print 'For Neuroticism, the most relevant features are:'\n",
    "#for trait in neuroRelevant:\n",
    "#    print dictAuTranslation[trait-1],\n",
    "#print \"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49  nearest neighbours is correct  74.4143330322 % of the time\n",
      "73.8060825053\n",
      "72  nearest neighbours is correct  76.1276723878 % of the time\n",
      "76.6230051189\n",
      "20  nearest neighbours is correct  75.1490514905 % of the time\n",
      "74.5408009636\n",
      "29  nearest neighbours is correct  74.1734417344 % of the time\n",
      "76.2541403192\n"
     ]
    }
   ],
   "source": [
    "opennessRelevant = backwardEliminationRE(1, 49)\n",
    "relevantKFoldCrossValidation(1, 49, opennessRelevant)\n",
    "print kFoldCrossValidation(1,49)\n",
    "\n",
    "extrRelevant = backwardEliminationRE(3,72)\n",
    "relevantKFoldCrossValidation(3, 72, extrRelevant)\n",
    "print kFoldCrossValidation(3,72)\n",
    "\n",
    "aggrRelevant = backwardEliminationRE(4,20)\n",
    "relevantKFoldCrossValidation(4, 20, aggrRelevant)\n",
    "print kFoldCrossValidation(4,20)\n",
    "\n",
    "neuroRelevant = backwardEliminationRE(4,29)\n",
    "relevantKFoldCrossValidation(4, 29, neuroRelevant)\n",
    "print kFoldCrossValidation(4,29)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'go' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-49-d89c13dd8604>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mxAxis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinspace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m trace1 = go.Scatter(\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mxAxis\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgraphData\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'go' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def gatherGraphData():\n",
    "    listOfTraces = []\n",
    "    for i in range(0,5):\n",
    "        listOfAccuraciesForK = []\n",
    "        for j in range(1,101):\n",
    "            listOfAccuraciesForK.append(kFoldCrossValidation(i, j))\n",
    "        listOfTraces.append(listOfAccuraciesForK)\n",
    "    return listOfTraces\n",
    "\n",
    "#print gatherGraphData()\n",
    "\n",
    "graphData = gatherGraphData()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~Karhu/12.embed\" height=\"525px\" width=\"100%\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import plotly.plotly as py\n",
    "import plotly.graph_objs as go\n",
    "\n",
    "xAxis = np.linspace(1,100,100)\n",
    "\n",
    "trace1 = go.Scatter(\n",
    "x = xAxis,\n",
    "y = graphData[0],\n",
    "mode = 'lines+markers',\n",
    "name = 'Openness'\n",
    ")\n",
    "\n",
    "trace2 = go.Scatter(\n",
    "x = xAxis,\n",
    "y = graphData[1],\n",
    "mode = 'lines+markers',\n",
    "name = 'Conscientiousness'\n",
    ")\n",
    "\n",
    "trace3 = go.Scatter(\n",
    "x = xAxis,\n",
    "y = graphData[2],\n",
    "mode = 'lines+markers',\n",
    "name = 'Extraversion'\n",
    ")\n",
    "\n",
    "trace4 = go.Scatter(\n",
    "x = xAxis,\n",
    "y = graphData[3],\n",
    "mode = 'lines+markers',\n",
    "name = 'Agreeableness'\n",
    ")\n",
    "\n",
    "trace5 = go.Scatter(\n",
    "x = xAxis,\n",
    "y = graphData[4],\n",
    "mode = 'lines+markers',\n",
    "name = 'Neuroticism'\n",
    ")\n",
    "    \n",
    "    \n",
    "data = [trace1, trace2, trace3, trace4, trace5]\n",
    "\n",
    "py.iplot(data, filename='line-mode')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
